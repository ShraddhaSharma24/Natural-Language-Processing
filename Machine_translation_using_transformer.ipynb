{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsMQIg1dnC9F2YeBd7NAYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShraddhaSharma24/Natural-Language-Processing/blob/main/Machine_translation_using_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-YPtLcszFBYH"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Transformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dummy Dataset"
      ],
      "metadata": {
        "id": "kFnMYE9lFi26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"i am a student\", \"je suis un étudiant\"),\n",
        "    (\"how are you\", \"comment ça va\"),\n",
        "    (\"hello\", \"bonjour\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"i love you\", \"je t'aime\"),\n",
        "    (\"good night\", \"bonne nuit\"),\n",
        "]\n"
      ],
      "metadata": {
        "id": "Nlx9bv9VFE_O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Basic whitespace tokenizer\n",
        "def tokenize(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "    idx = 4\n",
        "    for sentence in sentences:\n",
        "        for token in tokenize(sentence):\n",
        "            if token not in vocab:\n",
        "                vocab[token] = idx\n",
        "                idx += 1\n",
        "    return vocab\n",
        "\n",
        "# Prepare source and target sentences\n",
        "src_sentences = [pair[0] for pair in data]\n",
        "trg_sentences = [pair[1] for pair in data]\n",
        "\n",
        "SRC_vocab = build_vocab(src_sentences)\n",
        "TRG_vocab = build_vocab(trg_sentences)\n",
        "\n",
        "# Reverse mapping\n",
        "SRC_itos = {i: s for s, i in SRC_vocab.items()}\n",
        "TRG_itos = {i: s for s, i in TRG_vocab.items()}\n"
      ],
      "metadata": {
        "id": "WHt8oQu1Fk_b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(sentence, vocab, max_len=10):\n",
        "    tokens = tokenize(sentence)\n",
        "    token_ids = [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens]\n",
        "    token_ids = [vocab[\"<sos>\"]] + token_ids + [vocab[\"<eos>\"]]\n",
        "    token_ids = token_ids[:max_len] + [vocab[\"<pad>\"]] * (max_len - len(token_ids))\n",
        "    return torch.tensor(token_ids, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "NfgOUg-dFwDK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, src_vocab, trg_vocab):\n",
        "        self.data = data\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, trg = self.data[idx]\n",
        "        src_tensor = encode(src, self.src_vocab)\n",
        "        trg_tensor = encode(trg, self.trg_vocab)\n",
        "        return src_tensor, trg_tensor\n",
        "\n",
        "dataset = TranslationDataset(data, SRC_vocab, TRG_vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "6sqvdIdjFzx2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size, maxlen=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(maxlen, emb_size)\n",
        "        position = torch.arange(0, maxlen).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, emb_size, 2) * (-math.log(10000.0) / emb_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, maxlen, emb_size)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, emb_size, nhead, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "        self.trg_emb = nn.Embedding(trg_vocab_size, emb_size)\n",
        "        self.pos_enc = PositionalEncoding(emb_size)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.fc_out = nn.Linear(emb_size, trg_vocab_size)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # Shape: (batch, seq_len) -> (seq_len, batch, emb_size)\n",
        "        src = self.pos_enc(self.src_emb(src)).permute(1, 0, 2)\n",
        "        trg = self.pos_enc(self.trg_emb(trg)).permute(1, 0, 2)\n",
        "\n",
        "        src_mask = None  # optional: add masking here\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg.size(0)).to(trg.device)\n",
        "\n",
        "        output = self.transformer(src, trg, tgt_mask=trg_mask, src_mask=src_mask)\n",
        "        return self.fc_out(output).permute(1, 0, 2)  # (seq_len, batch, vocab) -> (batch, seq_len, vocab)\n"
      ],
      "metadata": {
        "id": "oDt7hzWIF2qc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_VOCAB_SIZE = len(SRC_vocab)\n",
        "TRG_VOCAB_SIZE = len(TRG_vocab)\n",
        "EMB_SIZE = 128\n",
        "NHEAD = 4\n",
        "HIDDEN_DIM = 512\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TransformerSeq2Seq(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, EMB_SIZE, NHEAD, HIDDEN_DIM, NUM_LAYERS).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JJhyblWGCZQ",
        "outputId": "03d50e64-8fcf-4389-81d0-5abacef8a1b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Ignore padding token in the loss\n",
        "TRG_PAD_IDX = TRG_vocab['<pad>']\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n"
      ],
      "metadata": {
        "id": "KURsDvC_Gd30"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, trg_pad_idx, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in dataloader:\n",
        "        src = src.to(device)  # shape: [src_len, batch_size]\n",
        "        trg = trg.to(device)  # shape: [trg_len, batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Output shape: [trg_len, batch_size, output_dim]\n",
        "        output = model(src, trg[:-1, :])  # input all tokens except <eos> (for training)\n",
        "\n",
        "        # Reshape to [trg_len * batch_size, output_dim]\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.reshape(-1, output_dim)\n",
        "        trg = trg[1:, :].reshape(-1)  # shift target by 1 (to predict next token)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "LwGrvzyzGHqd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, trg_pad_idx, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src = src.to(device)  # [src_len, batch_size]\n",
        "            trg = trg.to(device)  # [trg_len, batch_size]\n",
        "\n",
        "            output = model(src, trg[:-1, :])  # no teacher forcing\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.reshape(-1, output_dim)\n",
        "            trg = trg[1:, :].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "zZ6TbgcFHMF9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBmSkq_RHUvQ",
        "outputId": "5737c220-5fb2-40c2-f5a5-17a9b5709e2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gN4_HJVTV17",
        "outputId": "247e46ba-820d-4955-9c1e-37ae3924ee7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    tokens = [tok.lower() for tok in src_tokenizer(sentence)]\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "\n",
        "    src_indexes = [src_vocab[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.transformer.encoder(model.src_tok_emb(src_tensor) * math.sqrt(model.d_model))\n",
        "\n",
        "    trg_indexes = [trg_vocab['<sos>']]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(src_tensor, trg_tensor)\n",
        "            pred_token = output.argmax(2)[-1, :].item()\n",
        "\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(idx)] for idx in trg_indexes]\n",
        "    return trg_tokens[1:-1]  # remove <sos> and <eos>\n"
      ],
      "metadata": {
        "id": "4ObcSqsOUNE4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(data, model, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer, device):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    score = 0\n",
        "    total = 0\n",
        "\n",
        "    for src_sentence, trg_sentence in data:\n",
        "        pred_trg = translate_sentence(model, src_sentence, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer, device)\n",
        "        ref = [trg_tokenizer(trg_sentence)]\n",
        "\n",
        "        score += sentence_bleu(ref, pred_trg, smoothing_function=smoothie)\n",
        "        total += 1\n",
        "\n",
        "    return score / total\n"
      ],
      "metadata": {
        "id": "5s9Ug6JQUSEH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenizer(text):\n",
        "    return text.lower().strip().split()\n"
      ],
      "metadata": {
        "id": "Arl1IbUeUVFh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'i': 3, 'am': 4, 'a': 5, 'student': 6}\n",
        "TRG_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'je': 3, 'suis': 4, 'un': 5, 'étudiant': 6}\n",
        "\n",
        "SRC_vocab_inv = {v: k for k, v in SRC_vocab.items()}\n",
        "TRG_vocab_inv = {v: k for k, v in TRG_vocab.items()}\n"
      ],
      "metadata": {
        "id": "eb65MpcOUiTn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "    (\"I am a student\", \"Je suis un étudiant\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "qrYivLgRUj6S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = calculate_bleu(\n",
        "    test_data,\n",
        "    model=model,  # your trained model\n",
        "    src_vocab=SRC_vocab,\n",
        "    trg_vocab=TRG_vocab,\n",
        "    src_tokenizer=simple_tokenizer,\n",
        "    trg_tokenizer=simple_tokenizer,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "WFca_kroUmQY",
        "outputId": "4d6ac075-d5d0-4fd3-b45e-836035fb6b31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a1007f6fdd69>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m bleu_score = calculate_bleu(\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# your trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msrc_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSRC_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrg_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRG_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XsTpFUrRUovf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}