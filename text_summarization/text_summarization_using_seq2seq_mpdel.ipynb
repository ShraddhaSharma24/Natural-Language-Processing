{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhh8qhYGeHAhAIOIBMAabz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShraddhaSharma24/Natural-Language-Processing/blob/main/text_summarization_using_seq2seq_mpdel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1nuAZkRD48So"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import string\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tiny dataset of English â†’ Pig Latin words\n",
        "data = [\n",
        "    (\"hello\", \"ellohay\"),\n",
        "    (\"world\", \"orldway\"),\n",
        "    (\"python\", \"ythonpay\"),\n",
        "    (\"chat\", \"hatchay\"),\n",
        "    (\"bot\", \"otbay\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_chars = string.ascii_lowercase + \" \"  # 26 letters + space\n",
        "char2idx = {ch: i for i, ch in enumerate(all_chars)}\n",
        "idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "\n",
        "def word_to_tensor(word):\n",
        "    tensor = torch.zeros(len(word), 1, len(all_chars))\n",
        "    for li, letter in enumerate(word):\n",
        "        tensor[li][0][char2idx[letter]] = 1\n",
        "    return tensor.to(device)\n",
        "\n",
        "def tensor_to_word(tensor):\n",
        "    return ''.join([idx2char[i] for i in tensor])\n"
      ],
      "metadata": {
        "id": "bx7iHW2a5LPn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        hidden = torch.zeros(1, 1, self.hidden_size).to(device)\n",
        "        for i in range(input_seq.size(0)):\n",
        "            _, hidden = self.rnn(input_seq[i].unsqueeze(0), hidden)\n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "bppOyjys5b7r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(output_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, hidden, target_length):\n",
        "        input = torch.zeros(1, 1, len(all_chars)).to(device)\n",
        "        decoded_indices = []\n",
        "\n",
        "        for _ in range(target_length):\n",
        "            output, hidden = self.rnn(input, hidden)\n",
        "            output = self.softmax(self.out(output[0]))\n",
        "            topi = output.argmax(1)\n",
        "            decoded_indices.append(topi.item())\n",
        "\n",
        "            input = torch.zeros(1, 1, len(all_chars)).to(device)\n",
        "            input[0][0][topi.item()] = 1\n",
        "\n",
        "        return decoded_indices\n"
      ],
      "metadata": {
        "id": "esDmLTtq5e_d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderRNN(len(all_chars), 128).to(device)\n",
        "decoder = DecoderRNN(128, len(all_chars)).to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.01)\n",
        "\n",
        "# Train loop\n",
        "for epoch in range(500):\n",
        "    pair = random.choice(data)\n",
        "    input_tensor = word_to_tensor(pair[0])\n",
        "    target_tensor = torch.tensor([char2idx[c] for c in pair[1]], dtype=torch.long).to(device)\n",
        "\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "\n",
        "    hidden = encoder(input_tensor)\n",
        "    decoded = decoder(hidden, target_tensor.size(0))\n",
        "\n",
        "    loss = 0\n",
        "    for i in range(len(decoded)):\n",
        "        loss += criterion(torch.log_softmax(decoder.out.weight[decoded[i]].unsqueeze(0), dim=1), target_tensor[i].unsqueeze(0))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"[{epoch}] Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHH-ACW-5hYT",
        "outputId": "09091c5e-b907-4205-d8be-c6891452a066"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] Loss: 24.3709\n",
            "[100] Loss: 29.3675\n",
            "[200] Loss: 15.6784\n",
            "[300] Loss: 20.1264\n",
            "[400] Loss: 21.8515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(word):\n",
        "    input_tensor = word_to_tensor(word)\n",
        "    hidden = encoder(input_tensor)\n",
        "    decoded_indices = decoder(hidden, 10)\n",
        "    return tensor_to_word(decoded_indices)\n",
        "\n",
        "print(\"\\nðŸ”„ Translations:\")\n",
        "for eng, pig in data:\n",
        "    print(f\"{eng} â†’ {translate(eng)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwN0-kUh5lzB",
        "outputId": "13840b6f-426c-4e7f-d415-be9c67408659"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”„ Translations:\n",
            "hello â†’ hhhhhhhhhh\n",
            "world â†’ chhhhhhhhh\n",
            "python â†’ tuhhhhhhhh\n",
            "chat â†’ whhhhhhhhh\n",
            "bot â†’ thhhhhhhhh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v68CrP787PTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeHVhZMQ5xRp",
        "outputId": "3d965424-b2e1-4834-f7c0-0a05da01ecb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peek at the Data\n"
      ],
      "metadata": {
        "id": "NrucHqXD7S0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a small subset for training\n",
        "train_data = dataset['train']\n",
        "print(train_data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMmgFMag6PTm",
        "outputId": "2b774401-683d-459b-ac1b-d78577fb6f5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '0', 'translation': {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sentence pairs\n",
        "pairs = [(item['translation']['en'], item['translation']['fr']) for item in train_data]\n",
        "pairs = [pair for pair in pairs if pair[0] and pair[1]]  # Remove empty ones\n",
        "pairs = pairs[:10000]  # We'll use only 10k for quick training\n"
      ],
      "metadata": {
        "id": "hwr4Of6i6UMy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchtext\n",
        "!pip install torch==2.0.1 torchtext==0.15.2 --quiet\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4x9TixA6uhI",
        "outputId": "b62c1e7d-fd34-4487-8487-c73347215347"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.1\n",
            "Uninstalling torch-2.0.1:\n",
            "  Successfully uninstalled torch-2.0.1\n",
            "Found existing installation: torchtext 0.15.2\n",
            "Uninstalling torchtext-0.15.2:\n",
            "  Successfully uninstalled torchtext-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tokenize and Prepare Sequences"
      ],
      "metadata": {
        "id": "vA2yNcfG7H8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def tokenize(sentence):\n",
        "    return sentence.lower().strip().split()\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = build_vocab_from_iterator([tokenize(s) for s in sentences], specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    return vocab\n",
        "\n",
        "src_sentences = [src for src, tgt in pairs]\n",
        "tgt_sentences = [tgt for src, tgt in pairs]\n",
        "\n",
        "src_vocab = build_vocab(src_sentences)\n",
        "tgt_vocab = build_vocab(tgt_sentences)\n"
      ],
      "metadata": {
        "id": "gASJozej6YiG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Dummy English tokenized data (you can use real English-French pairs later)\n",
        "tokenized_en = [[\"hello\", \"world\"], [\"good\", \"morning\"], [\"how\", \"are\", \"you\"]]\n",
        "\n",
        "SRC_vocab = build_vocab_from_iterator(tokenized_en, specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
        "SRC_vocab.set_default_index(SRC_vocab[\"<unk>\"])\n",
        "\n",
        "INPUT_DIM = len(SRC_vocab)         # Now this will work\n",
        "ENC_EMB_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "\n",
        "# Example tokenized French sentences corresponding to the English ones\n",
        "tokenized_fr = [[\"bonjour\", \"le\", \"monde\"], [\"bon\", \"matin\"], [\"comment\", \"Ã§a\", \"va\"]]\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Add special tokens\n",
        "TRG_vocab = build_vocab_from_iterator(tokenized_fr, specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
        "TRG_vocab.set_default_index(TRG_vocab[\"<unk>\"])\n",
        "\n",
        "# Define output dimension\n",
        "OUTPUT_DIM = len(TRG_vocab)\n",
        "\n",
        "print(f\"TRG vocab size: {OUTPUT_DIM}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YokX_LJr-5_l",
        "outputId": "66157f73-ccf2-42e9-9fce-a68e37a0f5d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRG vocab size: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(SRC_vocab)      # Number of tokens in English vocab\n",
        "OUTPUT_DIM = len(TRG_vocab)     # Number of tokens in French vocab\n"
      ],
      "metadata": {
        "id": "KosPCgr0-9-j"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM)\n"
      ],
      "metadata": {
        "id": "SEdCPM-s_C4s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Tensors"
      ],
      "metadata": {
        "id": "JYrQlC1C7YxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numericalize(sentence, vocab):\n",
        "    tokens = [\"<sos>\"] + tokenize(sentence) + [\"<eos>\"]\n",
        "    return torch.tensor([vocab[token] for token in tokens], dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src, tgt in batch:\n",
        "        src_tensor = numericalize(src, src_vocab)\n",
        "        tgt_tensor = numericalize(tgt, tgt_vocab)\n",
        "        src_batch.append(src_tensor)\n",
        "        tgt_batch.append(tgt_tensor)\n",
        "    src_batch = pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"])\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab[\"<pad>\"])\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "train_dataloader = DataLoader(pairs, batch_size=32, shuffle=True, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "DOPBZiSj6l-G"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "XyBHLuDR9Vz0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)  # (1, batch_size)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))  # (batch_size, output_dim)\n",
        "        return prediction, hidden, cell\n"
      ],
      "metadata": {
        "id": "3lhauggV9tRY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = trg[0, :]  # <sos> token\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Ywm_j-Nc97O2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "INPUT_DIM = SRC_vocab\n",
        "OUTPUT_DIM = TRG_vocab\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "1my1UgBY9-pt",
        "outputId": "ebc3f323-bbff-413b-a6ac-a616b5f0a87a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4f5c1321867c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENC_EMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_EMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-0b1288c7a99c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, emb_dim, hidden_dim)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             self.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n\u001b[0m\u001b[1;32m    143\u001b[0m                                     requires_grad=not _freeze)\n\u001b[1;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"INPUT_DIM = {INPUT_DIM}\")\n",
        "print(f\"ENC_EMB_DIM = {ENC_EMB_DIM}\")\n",
        "print(f\"HIDDEN_DIM = {HIDDEN_DIM}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap4Y2Isf-BlX",
        "outputId": "f876ef1b-4088-49d0-9473-d56c134ffc5b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT_DIM = Vocab()\n",
            "ENC_EMB_DIM = 256\n",
            "HIDDEN_DIM = 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdZjyS-wAc1x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}